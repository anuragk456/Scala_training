{"cells":[{"cell_type":"code","execution_count":10,"id":"d9be78bc","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Marking org.apache.spark:spark-avro_2.12:3.3.2 for download\n","Obtained 12 files\n"]},{"data":{"text/plain":["lastException = null\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Marking org.apache.spark:spark-avro_2.12:3.3.2 for download\n","Obtained 12 files\n"]}],"source":["%AddDeps org.apache.spark spark-avro_2.12 3.3.2 --transitive"]},{"cell_type":"code","execution_count":11,"id":"3ac8978c","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["File transferred from gs://first-job-bucket/movies.csv to hdfs:///spark/Day16_17/CS4/movies.csv successfully!\n"]},{"data":{"text/plain":["spark = org.apache.spark.sql.SparkSession@108ea4e2\n","gcsPath = gs://first-job-bucket/movies.csv\n","hdfsPath = hdfs:///spark/Day16_17/CS4/movies.csv\n","data = [movieId: string, title: string ... 1 more field]\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[movieId: string, title: string ... 1 more field]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["import org.apache.spark.sql.SparkSession\n","\n","val spark = SparkSession.builder()\n","  .appName(\"Transfer File from GCS to HDFS\")\n","  .getOrCreate()\n","\n","val gcsPath = \"gs://first-job-bucket/movies.csv\"\n","val hdfsPath = \"hdfs:///spark/Day16_17/CS4/movies.csv\"\n","\n","val data = spark.read\n","  .option(\"header\", \"true\")\n","  .csv(gcsPath)\n","\n","data.write\n","  .option(\"header\", \"true\")\n","  .mode(\"overwrite\")\n","  .csv(hdfsPath)\n","\n","println(s\"File transferred from $gcsPath to $hdfsPath successfully!\")"]},{"cell_type":"code","execution_count":12,"id":"1e6538e5","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1000 duplicates inserted and file updated successfully!\n"]},{"data":{"text/plain":["moviesPath = hdfs:///spark/Day16_17/CS4/movies.csv\n","moviesDF = [movieId: string, title: string ... 1 more field]\n","sampleMoviesDF = [movieId: string, title: string ... 1 more field]\n","duplicateMoviesDF = [movieId: string, title: string ... 1 more field]\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[movieId: string, title: string ... 1 more field]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["val moviesPath = \"hdfs:///spark/Day16_17/CS4/movies.csv\"\n","val moviesDF = spark.read.option(\"header\", \"true\").csv(moviesPath)\n","\n","val sampleMoviesDF = moviesDF.limit(1000)\n","val duplicateMoviesDF = moviesDF.union(sampleMoviesDF)\n","\n","duplicateMoviesDF.write\n","  .option(\"header\", \"true\")\n","  .mode(\"overwrite\")\n","  .csv(\"hdfs:///spark/Day16_17/CS4/duplicated_movies.csv\")\n","\n","println(\"1000 duplicates inserted and file updated successfully!\")"]},{"cell_type":"code","execution_count":13,"id":"3b6c38e9","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Original record count: 88585\n","Deduplicated record count: 87585\n","Duplicates removed: 1000\n"]},{"data":{"text/plain":["moviesPath = hdfs:///spark/Day16_17/CS4/duplicated_movies.csv\n","moviesDF = [movieId: string, title: string ... 1 more field]\n","cleanedMoviesDF = [movieId: string, title: string ... 1 more field]\n","originalCount = 88585\n","deduplicatedCount = 87585\n","duplicatesRemoved = 1000\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["1000"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["val moviesPath = \"hdfs:///spark/Day16_17/CS4/duplicated_movies.csv\"\n","val moviesDF = spark.read.option(\"header\", \"true\").csv(moviesPath)\n","\n","val cleanedMoviesDF = moviesDF.rdd.map(row => {\n","  val movieId = row.getString(row.fieldIndex(\"movieId\"))\n","  val title = row.getString(row.fieldIndex(\"title\"))\n","  val genres = row.getString(row.fieldIndex(\"genres\"))\n","  ((movieId, title), genres)\n","}).reduceByKey((genres1, genres2) => s\"$genres1|$genres2\").map {\n","  case ((movieId, title), combinedGenres) => (movieId, title, combinedGenres)\n","}.toDF(\"movieId\", \"title\", \"genres\")\n","\n","val originalCount = moviesDF.count()\n","val deduplicatedCount = cleanedMoviesDF.count()\n","val duplicatesRemoved = originalCount - deduplicatedCount\n","println(s\"Original record count: $originalCount\")\n","println(s\"Deduplicated record count: $deduplicatedCount\")\n","println(s\"Duplicates removed: $duplicatesRemoved\")"]},{"cell_type":"code","execution_count":17,"id":"58a55053","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Cleaned movies data saved successfully in parquet format.\n"]},{"data":{"text/plain":["lastException = null\n","outputPath = gs://first-job-bucket/cleaned_movies.parquet\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["gs://first-job-bucket/cleaned_movies.parquet"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["val outputPath = s\"gs://first-job-bucket/cleaned_movies.parquet\"\n","cleanedMoviesDF.write\n","  .format(\"parquet\")\n","  .mode(\"overwrite\")\n","  .save(outputPath)\n","\n","println(\"Cleaned movies data saved successfully in parquet format.\")"]},{"cell_type":"code","execution_count":null,"id":"bece5e0e","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Apache Toree - Scala","language":"scala","name":"apache_toree_scala"},"language_info":{"codemirror_mode":"text/x-scala","file_extension":".scala","mimetype":"text/x-scala","name":"scala","pygments_lexer":"scala","version":"2.12.15"}},"nbformat":4,"nbformat_minor":5}