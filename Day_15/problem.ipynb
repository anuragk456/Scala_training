{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Exercise 1: Understanding RDD and Partitioning\n",
    "Objective: Create and manipulate an RDD while understanding its partitions.\n",
    "Task:\n",
    "\n",
    "Load a large text file (or create one programmatically with millions of random numbers).\n",
    "Perform the following:\n",
    "Check the number of partitions for the RDD.\n",
    "Repartition the RDD into 4 partitions and analyze how the data is distributed.\n",
    "Coalesce the RDD back into 2 partitions.\n",
    "Print the first 5 elements from each partition.\n",
    "Expected Analysis:\n",
    "\n",
    "View the effect of repartition and coalesce in the Spark UI (stages, tasks).\n",
    "\n",
    "===========================================================================\n",
    "\n",
    "Exercise 2: Narrow vs Wide Transformations\n",
    "Objective: Differentiate between narrow and wide transformations in Spark.\n",
    "Task:\n",
    "\n",
    "Create an RDD of numbers from 1 to 1000.\n",
    "Apply narrow transformations: map, filter.\n",
    "Apply a wide transformation: groupByKey or reduceByKey (simulate by mapping numbers into key-value pairs, e.g., (number % 10, number)).\n",
    "Save the results to a text file.\n",
    "Expected Analysis:\n",
    "\n",
    "Identify how narrow transformations execute within a single partition, while wide transformations cause shuffles.\n",
    "Observe the DAG in the Spark UI, focusing on stages and shuffle operations.\n",
    "\n",
    "=================================================================================\n",
    "Exercise 3: Analyzing Tasks and Executors\n",
    "Objective: Understand how tasks are distributed across executors in local mode.\n",
    "Task:\n",
    "\n",
    "Create an RDD of strings with at least 1 million lines (e.g., lorem ipsum or repetitive text).\n",
    "Perform a transformation pipeline:\n",
    "Split each string into words.\n",
    "Map each word to (word, 1).\n",
    "Reduce by key to count word occurrences.\n",
    "Set spark.executor.instances to 2 and observe task distribution in the Spark UI.\n",
    "Expected Analysis:\n",
    "\n",
    "Compare task execution times across partitions and stages in the UI.\n",
    "Understand executor and task allocation for a local mode Spark job.\n",
    "\n",
    "====================================================================================\n",
    "Exercise 4: Exploring DAG and Spark UI\n",
    "Objective: Analyze the DAG and understand the stages involved in a complex Spark job.\n",
    "Task:\n",
    "\n",
    "Create an RDD of integers from 1 to 10,000.\n",
    "Perform a series of transformations:\n",
    "\n",
    "Filter: Keep only even numbers.\n",
    "Map: Multiply each number by 10.\n",
    "Map: Generate a tuple where the first element is the remainder when dividing the number by 100 (key), and the second is the number itself (value).\n",
    "ReduceByKey: Group by the remainder (key) and compute the sum of the values.\n",
    "Finally, perform an action to collect the results and display them.\n",
    "Expected Analysis:\n",
    "\n",
    "Analyze the DAG generated for the job and how Spark breaks it into stages.\n",
    "Compare execution times of stages and tasks in the Spark UI.\n",
    "\n",
    "=================================================================================\n",
    "Exercise 5: Partitioning Impact on Performance\n",
    "Objective: Understand the impact of partitioning on performance and data shuffling.\n",
    "Task:\n",
    "\n",
    "Load a large dataset (e.g., a CSV or JSON file) into an RDD.\n",
    "Partition the RDD into 2, 4, and 8 partitions separately and perform the following tasks:\n",
    "Count the number of rows in the RDD.\n",
    "Sort the data using a wide transformation.\n",
    "Write the output back to disk.\n",
    "Compare execution times for different partition sizes.\n",
    "Expected Analysis:\n",
    "\n",
    "Observe how partition sizes affect shuffle size and task distribution in the Spark UI.\n",
    "Understand the trade-off between too many and too few partitions.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
